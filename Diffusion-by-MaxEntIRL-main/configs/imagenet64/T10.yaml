diffusion:
    sigma_min: 0.002
    sigma_max: 80.0
    image_size: 64
    num_channels: 192 
    num_res_blocks: 3
    num_heads: 4
    num_heads_upsample: -1
    num_head_channels: 64
    attention_resolutions: "32,16,8"
    channel_mult: ""
    dropout: 0.0
    class_cond: True 
    use_checkpoint: False
    use_scale_shift_norm: True
    resblock_updown: True 
    use_fp16: True 
    use_new_attention_order: False
    learn_sigma: False
    weight_schedule: "uniform"
    distillation: False  # boundary condition of consistency model

sampler:
  sample_shape: [3, 64, 64]
  n_timesteps: 10
  class_cond: True
  num_classes: 1000
  trainable_beta: fix_last
  sigma_min: 0.002
  sigma_max: 80.0

trainer:
  _target_: models.DxMI.trainer.DxMI_Trainer_Cond
  tau1: 0.1
  tau2: 0.01
  gamma: 1
  n_timesteps: 10
  use_sampler_beta: true
  adavelreg: 0.99
  entropy_in_value: null
  velocity_in_value: null
  value_grad_clip: true
  time_cost: 0
  skip_sampler_tau: 3
  time_cost_sig: 1


value:
  _target_: models.value.TimeIndependentValue
  net:
    _target_: models.modules.IGEBMEncoderV2
    in_chan: 3
    out_chan: 1
    use_spectral_norm: False
    keepdim: False
    out_activation: linear
    avg_pool_dim: 1
    learn_out_scale: True
    nh: 128


training:
  pretrained_path: pretrained/imagenet64_edm/edm_imagenet64_ema.pt
  # pretrained_path: ../consistency_models/pretrained/edm_imagenet64_ema.pt
  value_ckpt: Null
  n_iter: 10000
  batchsize: 128
  sampling_batchsize: 100
  n_fid_samples: 5000
  seed: 42
  lr: 1e-8
  v_lr: 1e-5
  beta_lr: 1.0e-06
  weight_decay: 0.
  initial_log_loss_scale: 20
  log_every: 20
  fid_every: 100

